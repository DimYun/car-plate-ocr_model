{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44792083-0ad2-4892-9562-93320298c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59e264-6794-4564-835d-5f4960bcc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append(path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231d29e-7a17-42fb-b3c0-a7e6ec3e191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from timm import create_model\n",
    "\n",
    "from src.transforms import PadResizeOCR, get_transforms\n",
    "from src.dataset import PlatesCodeDataset\n",
    "import jpeg4py as jpeg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor\n",
    "import albumentations as albu\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "import random\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3196f10478e0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(\n",
    "    img: NDArray[float],\n",
    "    mean: Tuple[float, ...] = (0.485, 0.456, 0.406),\n",
    "    std: Tuple[float, ...] = (0.229, 0.224, 0.225),\n",
    "    max_value: int = 255,\n",
    ") -> NDArray[int]:\n",
    "    denorm = albu.Normalize(\n",
    "        mean=[-me / st for me, st in zip(mean, std)],  # noqa: WPS221\n",
    "        std=[1.0 / st for st in std],\n",
    "        always_apply=True,\n",
    "        max_pixel_value=1.0,\n",
    "    )\n",
    "    denorm_img = denorm(image=img)['image'] * max_value\n",
    "    return denorm_img.astype(np.uint8)\n",
    "\n",
    "def tensor_to_cv_image(tensor: Tensor) -> NDArray[float]:\n",
    "    return tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "def text_decode(\n",
    "        text_vector: Tensor,\n",
    "        vocabular: str\n",
    ") -> str:\n",
    "    text_vector = text_vector.cpu().numpy()\n",
    "    text_list = [vocabular[x-1] for x in text_vector if x > 0]\n",
    "    return ''.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4545b2e99890a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='../data/'\n",
    "VOCAB = '#&0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZÄÅÖÜĆČĐŠŽБГДЗИЛПУЦЧЭЯ'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d5c41a547acc6",
   "metadata": {},
   "source": "## Check data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41957c8f9bfe1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = jpeg.JPEG('/home/dmitriy/Nextcloud/Projects/Proj_courses/DeepSchool/hw-02/model_plate-ocr/data/dataset-plates/kg/test/8074200.jpg').decode()\n",
    "image.shape\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70b4e0-03ec-4a3c-86a6-bcb732393266",
   "metadata": {},
   "source": "### Prepare data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3702527-73bd-4a6f-a816-c78e2943c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "DATA_FOLDER = '../data'\n",
    "\n",
    "train_dataset = PlatesCodeDataset(\n",
    "    phase='train', \n",
    "    data_folder=DATA_FOLDER,\n",
    "    reset_flag=True\n",
    ")\n",
    "print('\\n\\n')\n",
    "valid_dataset = PlatesCodeDataset(\n",
    "    phase='test', \n",
    "    data_folder=DATA_FOLDER,\n",
    "    reset_flag=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88f21fef0a7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_dataset)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ecd25-8150-469e-81d5-6b0e37338187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image sizes\n",
    "train_shapes = []\n",
    "for i in range(len(train_dataset)):\n",
    "    train_shapes.append(train_dataset[i][0].shape)\n",
    "train_shapes = np.array(train_shapes)\n",
    "\n",
    "valid_shapes = []\n",
    "for i in range(len(valid_dataset)):\n",
    "    valid_shapes.append(valid_dataset[i][0].shape)\n",
    "valid_shapes = np.array(valid_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eef3f1-ea7b-4641-b9f9-79b9a50082e2",
   "metadata": {},
   "source": "### Select height"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1749c41-061e-46c7-a514-dcc288203b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics and height distribution\n",
    "np.median(train_shapes[:, 0])\n",
    "_ = sns.distplot(train_shapes[:, 0])\n",
    "\n",
    "np.median(valid_shapes[:, 0])\n",
    "_ = sns.distplot(valid_shapes[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bdf872a8abc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics and wight distribution\n",
    "np.median(train_shapes[:, 1])\n",
    "_ = sns.distplot(train_shapes[:, 1])\n",
    "\n",
    "np.median(valid_shapes[:, 1])\n",
    "_ = sns.distplot(valid_shapes[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0389119-20f4-464b-b63c-59d9330736d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original crops\n",
    "for i in range(10):\n",
    "    Image.fromarray(train_dataset[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a785aab-2c0a-43aa-8a6c-ae0b683331fd",
   "metadata": {},
   "source": "We can see, that height is suitable, and we can use 64 for standardization (we need it dividable to 32 for pretrained backbone)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fbb19-d1a3-43b4-a9f0-eb92ac1647d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    image = train_dataset[i][0]\n",
    "    scale = 64 / image.shape[0]\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "    Image.fromarray(scaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978934c1-605e-4b5c-8ff8-374ffb4b79b8",
   "metadata": {},
   "source": [
    "Grete, we can clearly see each element with height 64.\n",
    "\n",
    "For OCR tasks, it is better to use resize with preserved aspect ratio and fill empty pixels with 0. Let's select weight for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2c488-809d-4be9-9ac5-f2338952dc06",
   "metadata": {},
   "source": "### Select weight"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287ca83-cbf5-4d6e-bf1a-38426e7bc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_width = train_shapes[:, 1] * 64/train_shapes[:, 0]\n",
    "valid_width = valid_shapes[:, 1] * 64/valid_shapes[:, 0]\n",
    "\n",
    "np.max(train_width)\n",
    "_ = sns.distplot(train_width)\n",
    "\n",
    "np.max(valid_width)\n",
    "_ = sns.distplot(valid_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76704686-95b5-4a5a-ae5e-90a4652c0a7f",
   "metadata": {},
   "source": "We select weight according to maximum value + some extra for possible future crops. We have here 1109 pixels as maximum value, but they are outlets, and we can use 416 pixels (also dividable to 32). Let's check."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1758a3157a7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(train_width))\n",
    "plt.imshow(Image.fromarray(train_dataset[np.argmax(train_width)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2d0eb4874edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shapes[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a70954ccc0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "_selected = np.array(train_dataset.image_paths)[train_width > 390]\n",
    "print(min(train_width[train_width > 390]))\n",
    "print(min(train_shapes[:, 1][train_width > 390]))\n",
    "print(len(_selected))\n",
    "for i in range(10):\n",
    "    image_path = _selected[i]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    scale = 64 / image.shape[0]\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "    Image.fromarray(scaled_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99ce9b432b8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(train_width))\n",
    "Image.fromarray(train_dataset[np.argmin(train_width)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc282f5f48a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_selected = np.array(train_dataset.image_paths)[train_width < 50]\n",
    "print(len(_selected))\n",
    "for i in range(10):\n",
    "    image_path = _selected[i]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    scale = 64 / image.shape[0]\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "    Image.fromarray(scaled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c36db1755605f",
   "metadata": {},
   "source": "So, according to height resize in 64 pixels, boundary conditions for weight are [50, 390] pixels. Our assumption about 416 pixel is valid."
  },
  {
   "cell_type": "markdown",
   "id": "d6669610-e62e-4b01-9fdf-7878cf26d582",
   "metadata": {},
   "source": "### Check final crops"
  },
  {
   "cell_type": "markdown",
   "id": "c215e233-96b6-4bd7-ad70-db831a9adb0b",
   "metadata": {},
   "source": "Finally, we can add some paddings to images, and we are ready to train! We can use *PadResizeOCR* self-written augmentation class."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea1b20-c944-4dd3-8e6d-a202407e670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PadResizeOCR(target_width=416, target_height=64, mode='left')\n",
    "\n",
    "for i in range(10):\n",
    "    image = train_dataset[i][0]\n",
    "    transformed_image = transform(image=image)['image']\n",
    "    Image.fromarray(transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc14328-e154-4e54-8c6b-cdf43341db57",
   "metadata": {},
   "source": "That is what we needed, we can train now."
  },
  {
   "cell_type": "markdown",
   "id": "eb24d593-a01f-4521-a3e6-ab00906608f1",
   "metadata": {},
   "source": "### Backbone selection"
  },
  {
   "cell_type": "markdown",
   "id": "5117e628-4302-4ed0-95e3-8376121cdb89",
   "metadata": {},
   "source": [
    "We need to select symbol weight according to feature map (how many \"gaps\" we use for single symbol prediction). Usually is recommended to use 2-3 \"gap\" per symbol.\n",
    "\n",
    "Thus, we have maximum 10 simbols in number with max weight 390 pixels: 390 / (10*3) = 13 pixels in \"gap\" (we can rounded that value if needed). But we use 416 pixel wight, so we need no less than 32 \"gaps\" (416 / 13 = 32.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495a21a-045e-4ead-b375-94fc97df4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check featuremap size after each layer\n",
    "backbone = create_model(\n",
    "            'resnet18',\n",
    "            pretrained=True,\n",
    "            features_only=True,\n",
    "            out_indices=(1,2,3,4),\n",
    "        )\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = backbone(torch.rand(1, 3, 64, 416))\n",
    "\n",
    "pred[0].shape\n",
    "pred[1].shape\n",
    "pred[2].shape\n",
    "pred[3].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb57c7-c203-4cbf-b517-faa46d989e4a",
   "metadata": {},
   "source": [
    "We need to use features from depper layers, according to our \"gap\" criterion:\n",
    "\n",
    "* 4th layer, wight 13 - too few\n",
    "* 3rd layer, wight 26 - we can use it, but, probably we'll get worst result\n",
    "* 2nd layer, wight 52 - suitable (32 < 52)\n",
    "* 1st layer, weight 104 - too much, and from 1st layer feature will include too few information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63faad4a-332d-4dcd-a276-b9dd608785d0",
   "metadata": {},
   "source": "If we need more deeper layers, we can use predefined backbone network and replace some straits from (2, 2) to (2, 1)."
  },
  {
   "cell_type": "markdown",
   "id": "6a80c745-57b7-4ecc-b5a6-f10c54ebc328",
   "metadata": {},
   "source": "Grete! We select backbone for our OCR model. If we need to tune other parameters, we can see sourse code for *CRNN*."
  },
  {
   "cell_type": "markdown",
   "id": "92628787669103d7",
   "metadata": {},
   "source": "## Check symbols encoding and augmentations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8b09efd1240f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_non_latin = (\n",
    "        \"АВ5СЕКХНМОРТУ0123456789ӨҮՈ\",\n",
    "        \"AB5CEKXHMOPTY0123456789&Y#\",\n",
    "    )\n",
    "\n",
    "tr_non_latin = {ord(a): ord(b) for a, b in zip(*symbols_non_latin)}\n",
    "\n",
    "\"а551оу750\".upper().translate(tr_non_latin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1c49a5e9d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_transforms = get_transforms(\n",
    "    width=416,\n",
    "    height=64,\n",
    "    vocab=VOCAB,\n",
    "    text_size=10,\n",
    "    postprocessing=True, \n",
    "    augmentations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006905422e5749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data'\n",
    "\n",
    "dataset = PlatesCodeDataset(\n",
    "    phase='train', \n",
    "    data_folder=DATA_FOLDER,\n",
    "    reset_flag=False,\n",
    "    transforms=_train_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7b1be2071c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1000000\n",
    "image, text, text_lenght, region = dataset[idx]\n",
    "text_str = text_decode(text, VOCAB)\n",
    "print(f'true: text = {text_str}, text_lenght = {text_lenght}, region = {region}')\n",
    "\n",
    "Image.fromarray(denormalize(tensor_to_cv_image(dataset[idx][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7c2a54b82930",
   "metadata": {},
   "source": "## Check Dataset, dataloader"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b0c802a1f86245",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_transforms = get_transforms(\n",
    "    width=416,\n",
    "    height=64,\n",
    "    vocab=VOCAB,\n",
    "    text_size=10,\n",
    ")\n",
    "_valid_transforms = get_transforms(\n",
    "    width=416,\n",
    "    height=64,\n",
    "    vocab=VOCAB,\n",
    "    text_size=10,\n",
    "    augmentations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ab16dc9e2b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = PlatesCodeDataset(\n",
    "    phase='train',\n",
    "    data_folder=DATA_PATH,\n",
    "    reset_flag=False,\n",
    "    transforms=_train_transforms,\n",
    ")\n",
    "valid_dataset = PlatesCodeDataset(\n",
    "    phase='test',\n",
    "    data_folder=DATA_PATH,\n",
    "    reset_flag=False,\n",
    "    transforms=_valid_transforms,\n",
    ")\n",
    "\n",
    "num_iterations = 100\n",
    "batch_size = 16\n",
    "if num_iterations != -1:\n",
    "    train_sampler = RandomSampler(\n",
    "        data_source=train_dataset,\n",
    "        num_samples=num_iterations * batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1a7d00fdbec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=10,\n",
    "    sampler=train_sampler,\n",
    "    shuffle=False if train_sampler else True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=10,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e364156b7f7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "for _test_dataset in [train_dataset, valid_dataset]:\n",
    "    random_i = random.randint(0, _test_dataset.__len__() - 1)\n",
    "    print(\n",
    "        f\"Selected index {random_i} from {_test_dataset.__len__()}. Image: {_test_dataset.image_paths[random_i]}\"\n",
    "    )\n",
    "    \n",
    "    image, text, text_lenght, region = _test_dataset[random_i]\n",
    "    text_str = text_decode(text, VOCAB)\n",
    "    print(f'true: text = {text_str}, text_lenght = {text_lenght}, region = {region}')\n",
    "    \n",
    "    Image.fromarray(denormalize(tensor_to_cv_image(_test_dataset[random_i][0])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83c4b4121498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataloader\n",
    "for _test_dataloader in [train_dl, valid_dl]:\n",
    "    print('process')\n",
    "    images, texts, text_lenghts, regions = next(iter(_test_dataloader))\n",
    "    text_str = text_decode(texts[0], VOCAB)\n",
    "    print(f'true: text = {text_str}, text_lenght = {text_lenghts[0]}, region = {regions[0]}')\n",
    "    \n",
    "    Image.fromarray(denormalize(tensor_to_cv_image(images[0])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "id": "30f6ee4d757f5604",
   "metadata": {},
   "outputs": [],
   "source": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
